#cloud-config

# sets the hostname to 'terraformed'
hostname: terraformed

write_files:
- path: /etc/netplan/20-internal-network.yaml
  content: |
    network:
      version: 2
      ethernets:
        "lo:0":
          match:
            name: lo
          dhcp4: false
          addresses:
          - 172.17.0.100/32
- path: /etc/netplan/10-user-network.yaml
  content: |
    network:
      version: 2
      ethernets:
        ens192:
          dhcp4: true
- path: /usr/bin/initconfig.sh
  content: |
    #!/bin/bash

    echo "



    InitConfig

    "

    docker run --rm \
            -v /var/lib/cloud/instance:/workdir \
            mikefarah/yq:3 \
            yq r user-data.txt glasswall.monitoring.username | tr '\r' '\n' > /home/ubuntu/monitoring-username.txt
    docker run --rm \
            -v /var/lib/cloud/instance:/workdir \
            mikefarah/yq:3 \
            yq r user-data.txt glasswall.monitoring.password | tr '\r' '\n' > /home/ubuntu/monitoring-password.txt
    docker run --rm \
            -v /var/lib/cloud/instance:/workdir \
            mikefarah/yq:3 \
            yq r user-data.txt glasswall.logging.username | tr '\r' '\n' > /home/ubuntu/logging-username.txt
    docker run --rm \
            -v /var/lib/cloud/instance:/workdir \
            mikefarah/yq:3 \
            yq r user-data.txt glasswall.logging.password | tr '\r' '\n' > /home/ubuntu/logging-password.txt
    docker run --rm \
            -v /var/lib/cloud/instance:/workdir \
            mikefarah/yq:3 \
            yq r user-data.txt glasswall.domain | tr '\r' '\n' > /home/ubuntu/domain.txt
    docker run --rm \
            -v /var/lib/cloud/instance:/workdir \
            mikefarah/yq:3 \
            yq r user-data.txt glasswall.service-cluster | tr '\r' '\n' > /home/ubuntu/service-cluster.txt
    docker run --rm \
            -v /var/lib/cloud/instance:/workdir \
            mikefarah/yq:3 \
            yq r user-data.txt glasswall.service-cluster-ip | tr '\r' '\n' > /home/ubuntu/service-cluster-ip.txt
    ec2metadata --instance-id > /home/ubuntu/cluster.txt
    cluster=$(cat /home/ubuntu/cluster.txt)
    monitoring_username=$(cat /home/ubuntu/monitoring-username.txt)
    monitoring_password=$(cat /home/ubuntu/monitoring-password.txt)
    logging_username=$(cat /home/ubuntu/logging-username.txt)
    logging_password=$(cat /home/ubuntu/logging-password.txt)
    domain=$(cat /home/ubuntu/domain.txt)
    service_cluster=$(cat /home/ubuntu/service-cluster.txt)
    service_cluster_ip=$(cat /home/ubuntu/service-cluster-ip.txt)

    chown -R 1000:1000 /mnt/disks/

    export KUBECONFIG=/etc/kubernetes/admin.conf
    wget https://raw.githubusercontent.com/k8-proxy/vmware-scripts/csapi-ck8-filedrop/packer/wc-coredns-configmap.yml -O /home/ubuntu/wc-coredns-configmap.yml
    sed -i "s|8.8.8.8|$service_cluster_ip|" /home/ubuntu/wc-coredns-configmap.yml
    kubectl apply -f /home/ubuntu/wc-coredns-configmap.yml
    kubectl delete pod --namespace kube-system --selector k8s-app=kube-dns
    rm /home/ubuntu/wc-coredns-configmap.yml

    cat /home/ubuntu/cluster.txt | xargs -I {} kubectl patch prometheuses.monitoring.coreos.com kube-prometheus-stack-prometheus -n monitoring --type='json' -p '[{"op": "replace", "path": "/spec/externalLabels/cluster", "value":"'{}'"}]'
    echo "https://influxdb.${service_cluster}/api/v1/prom/write?db=workload_cluster&u=${monitoring_username}&p=${monitoring_password}" > /home/ubuntu/influxdb-url.txt
    cat /home/ubuntu/influxdb-url.txt | xargs -I {} kubectl patch prometheuses.monitoring.coreos.com kube-prometheus-stack-prometheus -n monitoring --type='json' -p '[{"op": "replace", "path": "/spec/remoteWrite/0/url", "value":"'{}'"}]'
    kubectl delete pod -n monitoring prometheus-kube-prometheus-stack-prometheus-0

    kubectl get cm -n fluentd fluentd-fluentd-elasticsearch -o yaml | sed 's/CLUSTER_NAME/'"${cluster}"'/' | kubectl apply -f -
    kubectl get cm -n kube-system fluentd-system-fluentd-elasticsearch -o yaml | sed 's/CLUSTER_NAME/'"${cluster}"'/' | kubectl apply -f -
    kubectl set env daemonset.apps/fluentd-fluentd-elasticsearch -n fluentd OUTPUT_HOSTS=elastic.${service_cluster}
    kubectl set env daemonset.apps/fluentd-system-fluentd-elasticsearch -n kube-system OUTPUT_HOSTS=elastic.${service_cluster}
    kubectl set env daemonset.apps/fluentd-fluentd-elasticsearch -n fluentd OUTPUT_USER=${logging_username}
    kubectl set env daemonset.apps/fluentd-system-fluentd-elasticsearch -n kube-system OUTPUT_USER=${logging_username}
    cat /home/ubuntu/logging-password.txt | xargs -I {} kubectl patch secret -n fluentd elasticsearch --type='json' -p '[{"op": "replace", "path": "/data/password", "value":"'{}'"}]'
    cat /home/ubuntu/logging-password.txt | xargs -I {} kubectl patch secret -n kube-system elasticsearch --type='json' -p '[{"op": "replace", "path": "/data/password", "value":"'{}'"}]'

    while [ ! -d /mnt/disks/prometheus-db ]; do sleep 10; done
    chown -R 1000:1000 /mnt/disks/prometheus-db

    > /home/ubuntu/monitoring-username.txt
    > /home/ubuntu/monitoring-password.txt
    > /home/ubuntu/logging-username.txt
    > /home/ubuntu/logging-password.txt
    > /home/ubuntu/service-cluster.txt
    > /home/ubuntu/service-cluster-ip.txt
    > /home/ubuntu/cluster.txt
    > /home/ubuntu/domain.txt

    exit

- path: /etc/systemd/system/initconfig.service
  content: |
    [Unit]
    Description=InitConfig
    After=kubelet.service

    [Service]
    Type=oneshot
    ExecStartPre=kubectl get pods --kubeconfig=/etc/kubernetes/admin.conf -A
    ExecStart=/usr/bin/initconfig.sh

    RemainAfterExit=yes
    TimeoutSec=0

    Restart=on-failure
    RestartSec=30

    # Output needs to appear in instance console output
    StandardOutput=journal+console
    StandardError=journal+console
    SyslogIdentifier=initconfig

    [Install]
    WantedBy=cloud-init.target

- path: /etc/systemd/system/initconfig-prometheus.service
  content: |
    [Unit]
    Description=InitConfigPrometheus
    After=initconfig.service

    [Service]
    Type=oneshot
    ExecStartPre=test -d /mnt/disks/prometheus-db
    ExecStart=chown -R 1000:1000 /mnt/disks/prometheus-db

    RemainAfterExit=yes
    TimeoutSec=0

    Restart=on-failure
    RestartSec=30

    # Output needs to appear in instance console output
    StandardOutput=journal+console
    StandardError=journal+console
    SyslogIdentifier=initconfig

    [Install]
    WantedBy=cloud-init.target

runcmd:
  - netplan apply
  - sleep 5
  - ifconfig -a
  - route -n
  - chmod a+r /var/lib/cloud/instance/user-data.txt
  - chmod 755 /usr/bin/initconfig.sh
  - systemctl daemon-reload
  - systemctl enable initconfig